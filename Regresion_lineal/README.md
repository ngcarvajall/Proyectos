
# üè° Modelos Predictivos: Predicci√≥n de Precios de Propiedades

## üìñ Descripci√≥n
En este proyecto trabaj√© con **modelos predictivos** para estimar precios de propiedades. Utilic√© diversas t√©cnicas de an√°lisis de datos y Machine Learning, aplicando transformaciones avanzadas y ajustes de par√°metros para optimizar los resultados. Durante el proceso, evalu√© c√≥mo las decisiones en el preprocesamiento y ajuste de par√°metros afectaban el rendimiento de los modelos.

---

## üóÇÔ∏è Estructura del Proyecto

```
‚îú‚îÄ‚îÄ Datos/                     # Contiene los datos iniciales y procesados
‚îú‚îÄ‚îÄ Modelos/                   # Contiene los modelos en desarrollo
‚îú‚îÄ‚îÄ modelos_pickle/            # Modelos y transformadores guardados como pickle
‚îú‚îÄ‚îÄ Notebooks/                 # Notebooks de Jupyter con el an√°lisis exploratorio
‚îú‚îÄ‚îÄ src/                       # Scripts de procesamiento y modelado
‚îú‚îÄ‚îÄ Foto/                      # Im√°genes para la visualizaci√≥n del proyecto
‚îú‚îÄ‚îÄ README.md                  # Este archivo de descripci√≥n
```

---

## üõ†Ô∏è Instalaci√≥n y Requisitos
Este proyecto utiliza **Python 3.8+**. Aseg√∫rate de instalar las siguientes dependencias. Los enlaces llevan a la documentaci√≥n oficial de cada paquete:

- [pandas](https://pandas.pydata.org/docs/)
- [numpy](https://numpy.org/doc/)
- [matplotlib](https://matplotlib.org/stable/contents.html)
- [seaborn](https://seaborn.pydata.org/)
- [scikit-learn](https://scikit-learn.org/stable/documentation.html)
- [xgboost](https://xgboost.readthedocs.io/)
- [streamlit](https://docs.streamlit.io/)

Para instalar todos los requisitos, usa el siguiente comando:

```bash
pip install -r requirements.txt
```

---

## ‚öôÔ∏è Detalles del An√°lisis

### **Modelo Inicial:**
1. **Limpieza de Datos:**
   - Eliminaci√≥n de columnas innecesarias.
   - Transformaci√≥n de variables a formatos m√°s √∫tiles.
   - Agregaci√≥n de nuevas categor√≠as para variables categ√≥ricas.

2. **Procesamiento de Datos:**
   - **Estandarizaci√≥n num√©rica:** Aplicaci√≥n del m√©todo est√°ndar.
   - **Detecci√≥n de Outliers:** Usando Isolation Forest con los siguientes par√°metros:
     - Contaminaci√≥n: `[0.01, 0.05, 0.1]`
     - Estimadores: `[10, 100, 150]`
     - Selecci√≥n de los elementos se√±alados al 100% como outliers y algunos dentro del fragmento del 60%.
   - **Codificaci√≥n de Variables Categ√≥ricas:** 
     - M√©todos estad√≠sticos: `mannwhitneyu` y `kruskal`.
     - Aplicaci√≥n posterior de **One-Hot Encoding** y **Target Encoding**.

3. **Modelos Probados:**
   - **Decision Tree**
   - **Random Forest**
   - **Gradient Booster**
   - **XGBooster**

   Se realizaron pruebas con el 70% y 80% de datos de entrenamiento y ajustes de par√°metros espec√≠ficos.

### **Iteraciones en Modelos Posteriores:**
- Se prob√≥ con diferentes √≥rdenes de transformaciones para observar el impacto en las m√©tricas finales.
- Evaluaci√≥n continua de combinaciones de par√°metros para optimizar el rendimiento.

---

## üìä Resultados y Conclusiones

- **Eficiencia de Preprocesamiento:** La integraci√≥n de One-Hot Encoding y Target Encoding permiti√≥ capturar relaciones categ√≥ricas importantes.
- **Impacto de la Detecci√≥n de Outliers:** La eliminaci√≥n de ruido en los datos mediante Isolation Forest mejor√≥ la precisi√≥n de los modelos.
- **Modelos Predictivos:** Los modelos lograron un desempe√±o estable con m√©tricas confiables tras las iteraciones de ajustes.

---

## üîÑ Pr√≥ximos Pasos
- Continuar optimizando los hiperpar√°metros para mejorar las m√©tricas.
- Experimentar con modelos avanzados como **LightGBM** o **CatBoost** para comparar resultados.
- Explorar nuevas t√©cnicas de **feature engineering** para variables categ√≥ricas y num√©ricas.

---

## üì∑ Visualizaci√≥n de Datos
Aqu√≠ tienes una muestra de la distribuci√≥n de los datos utilizada durante el an√°lisis:

![Distribuci√≥n de Datos](Foto/distribucion_datos.png)

---

## ‚úíÔ∏è Autor
- **Nelson Carvajal** - [GitHub](https://github.com/ngcarvajall)
